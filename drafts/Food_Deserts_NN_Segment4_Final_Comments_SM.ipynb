{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UTMCC DataViz Module 20 Team Project --  \n",
    "## Neural Network Machine Deep Learning Model  \n",
    "### Food Deserts in the Austin, Texas Metro Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import activations\n",
    "# Import checkpoint dependencies\n",
    "import os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'psycopg2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-01de9408c2d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mio\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpsycopg2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'psycopg2'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import datetime as dt\n",
    "import calendar\n",
    "import random\n",
    "from path import Path\n",
    "\n",
    "import io\n",
    "import sys\n",
    "import psycopg2\n",
    "import csv\n",
    "import codecs\n",
    "import boto3\n",
    "import itertools\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import style\n",
    "style.use('fivethirtyeight')\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics\n",
    "# from flask import Flask, jsonify\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "# from hvplot import hvPlot\n",
    "# import hvplot.pandas\n",
    "# import plotly.express as px\n",
    "\n",
    "# Python SQL toolkit and Object Relational Mapper\n",
    "import sqlite3\n",
    "import sqlalchemy\n",
    "from sqlalchemy.ext.automap import automap_base\n",
    "from sqlalchemy.orm import Session\n",
    "from sqlalchemy import create_engine, func, select, delete, Table\n",
    "from sqlalchemy import extract\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and read csv file from AWS S3 Bucket using Boto 3 and Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download csv files from AWS S3 and create a pandas dataframe \n",
    "\n",
    "client = boto3.client('s3', 'us-east-2', aws_access_key_id='xxxxxxxx', \n",
    "                                  aws_secret_access_key='xxxxxxxxxy')\n",
    "\n",
    "obj = client.get_object(Bucket= \"dataviz20-bucket\", Key= \"food_access_research_atlas.csv\") \n",
    "\n",
    "food_atlas_df = pd.read_csv(io.BytesIO(obj['Body'].read()), encoding='utf8')\n",
    "food_atlas_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client('s3', 'us-east-2', aws_access_key_id='xxxxxxxx', \n",
    "                                  aws_secret_access_key='xxxxxxxxy')\n",
    "\n",
    "obj = client.get_object(Bucket= \"dataviz20-bucket\", Key= \"food_desert_austin_censustract.csv\") \n",
    "\n",
    "fooddesert_austin_censustract_df = pd.read_csv(io.BytesIO(obj['Body'].read()), encoding='utf8')\n",
    "# fooddesert_austin_censustract_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client('s3', 'us-east-2', aws_access_key_id='xxxxxxxx', \n",
    "                                  aws_secret_access_key='xxxxxxxxy')\n",
    "\n",
    "obj = client.get_object(Bucket= \"dataviz20-bucket\", Key= \"census_tract_shapefiles_all.csv\") \n",
    "\n",
    "census_tract_shapefiles_all_df = pd.read_csv(io.BytesIO(obj['Body'].read()), encoding='utf8')\n",
    "# census_tract_shapefiles_all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename column GEOID to CensusTract\n",
    "census_tract_shapefiles_all_df = census_tract_shapefiles_all_df.rename(columns={\"GEOID\":\"CensusTract\"})\n",
    "# census_tract_shapefiles_all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and read csv files\n",
    "# food_atlas_df = pd.read_csv(\"resources/food_access_research_atlas.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# food_atlas_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_atlas_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values using isnull()\n",
    "food_atlas_df.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows that may have null, missing values.\n",
    "rows_len_nan_check = food_atlas_df.dropna(how='all')\n",
    "rows_len_nan_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare sizes of the dataframes to indicate how many rows had a minimum of one null value. \n",
    "print(\"Old data frame length:\", len(food_atlas_df)) \n",
    "print(\"New data frame length:\", len(rows_len_nan_check))  \n",
    "print(\"Number of rows with at least 1 NA value: \", (len(food_atlas_df)-len(rows_len_nan_check))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new df keeping only Texas\n",
    "food_texas_df = food_atlas_df[(food_atlas_df[\"State\"]==\"Texas\")]\n",
    "food_texas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new df keeping only select Counties in the Austin Metro Area\n",
    "counties = [\"Bastrop\", \"Caldwell\", \"Hays\", \"Travis\", \"Williamson\"]\n",
    "food_austin_df = food_texas_df.loc[food_texas_df[\"County\"].isin(counties)]\n",
    "food_austin_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a file for visualization, food desert tracts for Austin Metro Area  food_austin_df.loc\n",
    "# aus_desert_tracts = [\"CensusTract\", \"LILATracts_1And10\"]\n",
    "LILATracts_1And10_aus_df = food_austin_df[[\"CensusTract\", \"LILATracts_1And10\"]]\n",
    "LILATracts_1And10_aus_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to csv \n",
    "# LILATracts_1And10_aus_df.to_csv(\"LILATracts_1And10_aus.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin Income column creation (target variable), MFI = MedianFamilyIncome']\n",
    "food_austinMFI_df = food_austin_df\n",
    "food_austinMFI_df['MedianFamilyIncome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Poverty Guidelines U.S. 2015 family of 4 is $24250\n",
    "conditions = [(food_austinMFI_df['MedianFamilyIncome'] <= 24250), \n",
    "              (food_austinMFI_df['MedianFamilyIncome'] > 24250)]\n",
    "values = [0, 1]\n",
    "food_austinMFI_df[\"Income\"] = np.select(conditions, values)\n",
    "food_austinMFI_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_austinMFI_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to csv \n",
    "# food_austinMFI_df.to_csv(\"food_desert_austinMFI.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new df with select feature columns representing \"share\" values\n",
    "food_desert_Austin_df = food_austinMFI_df[[\"Income\", \"lasnaphalfshare\", \"lahunvhalfshare\", \"lasnap1share\", \"lahunv1share\", \"lasnap10share\", \"lahunv10share\", \"lasnap20share\", \"lahunv20share\"]]\n",
    "food_desert_Austin_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to csv \n",
    "# food_desert_Austin_df.to_csv(\"food_desert_Austin.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing for: ML Training on full U.S. Census dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation for full U.S. dataset\n",
    "# Begin Income column creation (target variable), MFI = MedianFamilyIncome\n",
    "food_atlasMFI_df = food_atlas_df\n",
    "food_atlasMFI_df['MedianFamilyIncome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Poverty Guidelines U.S. 2015 family of 4 is $24,250\n",
    "conditions = [(food_atlasMFI_df['MedianFamilyIncome'] <= 24250), \n",
    "              (food_atlasMFI_df['MedianFamilyIncome'] > 24250)]\n",
    "values = [0, 1]\n",
    "food_atlasMFI_df[\"Income\"] = np.select(conditions, values)\n",
    "food_atlasMFI_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_atlasMFI_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to csv \n",
    "# food_atlasMFI_df.to_csv(\"food_atlasMFI.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new df with select feature columns for all of U.S. (not only for Texas), with 72,864 rows\n",
    "# Use this df as ML Training set, this input data has no categorical data types, it can be provided to the neural network model in its raw form \n",
    "food_desertUS_df = food_atlasMFI_df[[\"Income\", \"lasnaphalfshare\", \"lahunvhalfshare\", \"lasnap1share\", \"lahunv1share\", \"lasnap10share\", \"lahunv10share\", \"lasnap20share\", \"lahunv20share\"]]\n",
    "food_desertUS_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to csv\n",
    "# food_desertUS_df.to_csv(\"food_desertUS.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a OneHotEncoder instance for column Income, and although the Income data value are numerical, not categorical, \n",
    "#  to ensure that the values are encoded for the ML model.\n",
    "enc = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Fit and transform the OneHotEncoder using the categorical variable list\n",
    "encode_df = pd.DataFrame(enc.fit_transform(food_desertUS_df.Income.values.reshape(-1,1)))\n",
    "encode_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to csv\n",
    "# encode_df.to_csv(\"encode.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Models\n",
    "### Training: on full U.S. Census dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the preprocessed dataframe into our features and target arrays\n",
    "#  Remove Income target from features data. Establish the target output, y, as the encoded Income column for \"1\".\n",
    "#   The two columns of the endoce_df are redundant to each other, as they are dichotomous, we only need one of the colunns.\n",
    "y = encode_df[1]\n",
    "# y = food_desertUS_df[\"Income\"]\n",
    "X = food_desertUS_df.drop(columns=\"Income\").values\n",
    "# Split the preprocessed data into a training and testing dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create StandardScaler instance. Using the StandardScaler module to standardize our numerical variables, we reduce the overall\n",
    "#  likelihood that outliers, variables of different units, or skewed distributions will have a negative impact on the model's performance.\n",
    "scaler = StandardScaler()\n",
    "# Fit the StandardScaler\n",
    "X_scaler = scaler.fit(X_train)\n",
    "# Scale the data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model - deep neural net. the number of input features and the hidden nodes for each layer.\n",
    "# A rule of thumb for a basic neural network is to have two to three times the amount of neurons in \n",
    "# the hidden layer as the number of inputs.\n",
    "number_input_features = len(X_train[0])\n",
    "hidden_nodes_layer1 = 30\n",
    "hidden_nodes_layer2 = 9\n",
    "\n",
    "nn = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\"))\n",
    "\n",
    "# Second hidden layer\n",
    "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
    "\n",
    "# Output layer\n",
    "nn.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Check the structure of the model\n",
    "nn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the checkpoint path and filenames\n",
    "os.makedirs(\"checkpoints/\",exist_ok=True)\n",
    "checkpoint_path = \"checkpoints/weights.{epoch:02d}.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and train the model\n",
    "nn.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Create a callback that saves the model's weights every 5 epochs. Checkpoints will be saved every thousand samples tested (across all epochs).\n",
    "# Using the Keras ModelCheckpoint method\n",
    "cp_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    verbose=1,\n",
    "    save_weights_only=True,\n",
    "    save_freq=1000)\n",
    "\n",
    "# Train the model\n",
    "fit_model = nn.fit(X_train_scaled, y_train, epochs=100, callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model using the test data, on the full U.S. Training set.\n",
    "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy*100} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hierarchical Data Format file, HDF5. Using the Keras Sequential model's save method to export the model.\n",
    "# Export our model to HDF5 file\n",
    "nn.save(\"trained_food_desertUS.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Confusion Matrix for Training Data - full U.S. Census Tracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create predictions to compare in the CM\n",
    "predictions_train= nn.predict(X_train)\n",
    "predictions_test= nn.predict(X_test)\n",
    "# Check shape of test array\n",
    "predictions_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Round predicions into binaries for comparisons to y data\n",
    "pred_test_round = predictions_test.round()\n",
    "pred_test_round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check shape of array\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape array into 1-D\n",
    "y_test_reshape = y_test.values.reshape(18216,1)\n",
    "y_test_reshape.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recheck shape of array\n",
    "y_test_reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy score to verify CM findings\n",
    "acc_score= accuracy_score(y_test_reshape, pred_test_round, normalize = False)\n",
    "acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix (y_test_reshape, pred_test_round))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix (y_test_reshape, pred_test_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                        normalize=False,\n",
    "                        title='Confusion matrix',\n",
    "                        cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "            horizontalalignment=\"center\",\n",
    "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_plot_labels = ['Below Poverty Level','Above Poverty Level']\n",
    "plot_confusion_matrix(cm=cm, classes=cm_plot_labels, title='Confusion Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing: Using the Training results from the full U.S. Trained weights on the Austin-Metro only data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NN Machine Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform the OneHotEncoder using the categorical variable list\n",
    "encodeAus_df = pd.DataFrame(enc.fit_transform(food_desert_Austin_df.Income.values.reshape(-1,1)))\n",
    "encodeAus_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the preprocessed dataframe into our features and target arrays\n",
    "#  Remove Income target from features data\n",
    "y = encodeAus_df[1]\n",
    "X = food_desert_Austin_df.drop(columns=\"Income\").values\n",
    "\n",
    "# Split the preprocessed data into a training and testing dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create StandardScaler instances\n",
    "scaler = StandardScaler()\n",
    "# Fit the StandardScaler\n",
    "X_scaler = scaler.fit(X_train)\n",
    "# Scale the data\n",
    "X_train_scaled = X_scaler.transform(X_train)\n",
    "X_test_scaled = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model - deep neural net. the number of input features and the hidden nodes for each layer.\n",
    "number_input_features = len(X_train[0])\n",
    "hidden_nodes_layer1 = 20\n",
    "hidden_nodes_layer2 = 10\n",
    "\n",
    "nn_new = tf.keras.models.Sequential()\n",
    "\n",
    "# First hidden layer\n",
    "nn_new.add(tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\"))\n",
    "\n",
    "# Second hidden layer\n",
    "nn_new.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
    "\n",
    "# Output layer\n",
    "nn_new.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
    "\n",
    "# Check the structure of the model\n",
    "nn_new.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore the model weights\n",
    "nn_new = tf.keras.models.load_model('trained_food_desertUS.h5')\n",
    "\n",
    "# Compile the model\n",
    "nn_new.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "# fit_model = nn_new.fit(X_train_scaled,y_train,epochs=50)\n",
    "\n",
    "# Evaluate the model using the test data,on the Austin-Metro only Test set.\n",
    "model_loss, model_accuracy = nn_new.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy*100} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hierarchical Data Format file, HDF5. Using the Keras Sequential model's save method to export the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the new model to HDF5 file\n",
    "nn_new.save(\"trained_food_desert_Austin.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying the saved h5 file to recreate, check, and test for performance. \n",
    "# Import the model to a new object\n",
    "nn_new_imported = tf.keras.models.load_model('trained_food_desert_Austin.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model using the test data\n",
    "model_loss, model_accuracy = nn_new.evaluate(X_test_scaled,y_test,verbose=2)\n",
    "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy*100} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure connection and engine for AWS RDS with SQLAlchemy, and \n",
    "###   Writing to the PostgreSQL database: Module20_food_deserts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure connection and engine for AWS RDS with SQLAlchemy.\n",
    "connection = psycopg2.connect(\n",
    "    host = 'dataviz.c5qcqhh5xq62.us-east-2.rds.amazonaws.com',\n",
    "    port = 5432,\n",
    "    user = 'postgres',\n",
    "    password = 'xxxxx',\n",
    "    database= 'Module20_food_deserts'\n",
    "    )\n",
    "cursor=connection.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creat engine for postgresql.\n",
    "engine = create_engine('postgresql://postgres:Data1UT$@dataviz.c5qcqhh5xq62.us-east-2.rds.amazonaws.com/Module20_food_deserts')\n",
    "con = engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LILATracts_1And10_aus_df.to_sql('LILATracts_1And10_aus', engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_austinMFI_df.to_sql('food_austinMFI', engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_desert_Austin_df.to_sql('food_desert_Austin', engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_atlasMFI_df.to_sql('food_atlasMFI', engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_desertUS_df.to_sql('food_desertUS', engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LILATracts_1And10_aus_df\n",
    "# food_austinMFI_df\n",
    "# food_desert_Austin_df\n",
    "# food_atlasMFI_df\n",
    "# food_desertUS_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join two of the DataFrames\n",
    "fooddesert_austin_censusshapes_df = fooddesert_austin_censustract_df.merge(census_tract_shapefiles_all_df, on=\"CensusTract\", how=\"inner\")\n",
    "#fooddesert_austin_censusshapes_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL LEFT JOIN \n",
    "sql = \"SELECT \\\n",
    "   fa.CensusTract, \\\n",
    "    fa.LowIncomeTracts, \\\n",
    "    fa.PovertyRate, \\\n",
    "    fa.MedianFamilyIncome, \\\n",
    "    fa.Income, \\\n",
    "    li.LILATracts_1And10 \\\n",
    "FROM food_austinMFI AS fa \\\n",
    "LEFT JOIN LILATracts_1And10_aus AS li ON fa.CensusTract = li.CensusTract\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cursor.execute(sql)\n",
    "# myresult = cursor.fetchall()\n",
    "# for x in myresult:\n",
    "#    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Confusion Matrix for Test Data - Austin Metro Area Food Deserts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create predictions to compare in the CM\n",
    "predictions_train= nn_new.predict(X_train)\n",
    "predictions_test= nn_new.predict(X_test)\n",
    "# Check shape of test array\n",
    "predictions_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View state of prediction data\n",
    "predictions_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Round predictions into binaries for comparisons to y data\n",
    "pred_test_round = predictions_test.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check shape of array\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reshape array into 1-D\n",
    "y_test_reshape = y_test.values.reshape(88,1)\n",
    "y_test_reshape.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recheck shape of array\n",
    "y_test_reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy score to verify CM findings\n",
    "acc_score= accuracy_score(y_test_reshape, pred_test_round, normalize = False)\n",
    "acc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix (y_test_reshape, pred_test_round))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix (y_test_reshape, pred_test_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                        normalize=False,\n",
    "                        title='Confusion matrix',\n",
    "                        cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "            horizontalalignment=\"center\",\n",
    "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_plot_labels = ['Below Poverty Level','Above Poverty Level']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(cm=cm, classes=cm_plot_labels, title='Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
